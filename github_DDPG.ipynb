{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "github DDPG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPLUKEdT08Co"
      },
      "source": [
        "##DDPG "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6Tx2ERn53ZP",
        "outputId": "1cbd702e-08ff-4165-86b2-b4c46f95aa3f"
      },
      "source": [
        "#https://stackoverflow.com/questions/64161280/rl-problem-on-colab-for-gym-envs-box2d-has-no-attribute-lunarlander\n",
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "import gym\n",
        "env = gym.make(\"LunarLanderContinuous-v2\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 112 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 448 kB 18.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OXafnLTZgl3"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import os\n",
        "import copy \n",
        "np.set_printoptions(suppress=True)\n",
        "random.seed(123); np.random.seed(123)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThcRnhSx07Rq"
      },
      "source": [
        "#https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py\n",
        "\n",
        "class OUActionNoise():\n",
        "  def __init__(self, mu, sigma=0.15, theta=0.2, dt=1e-2, x0=None):\n",
        "    self.mu = mu\n",
        "    self.theta = theta\n",
        "    self.dt = dt\n",
        "    self.x0 = x0  #starting value\n",
        "    self.sigma = sigma\n",
        "    self.reset()  #reset noise at the top of each episode (reset noise) at our constructor\n",
        "  \n",
        "  def __call__(self):\n",
        "    #What __call__ enables us to do:\n",
        "    #noise = OUActionNoise()  \n",
        "    #current_noise = noise()\n",
        "\n",
        "    #GET TEMPORAL correlation of noise\n",
        "    x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "    self.sigma  * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "  \n",
        "    self.x_prev = x\n",
        "    return x\n",
        "\n",
        "  def reset(self):\n",
        "    self.x_prev = self.xo if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhPFBDQh1Chz"
      },
      "source": [
        "#THIS CODE along with the rest of the enviro can be used for ANY atari\n",
        "#game since it is totally generic - copied from DQN course since I 've already \n",
        "#dedicated a significant amount of time to write this in the first place\n",
        "class ReplayBuffer():\n",
        "  def __init__(self, max_size, input_shape, n_actions):\n",
        "    self.mem_size = max_size \n",
        "\n",
        "    #position of last stored memory\n",
        "    self.mem_cntr = 0\n",
        "\n",
        "    #input_shape is for handling states of variable length \n",
        "    #DTYPE is for pytorch particulars plus 64 bit too much memory\n",
        "    self.state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float32)\n",
        "    \n",
        "    self.new_state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float32)\n",
        "\n",
        "    #n_Actions here really is the #of componenets per action, here in LunarLander we have a vector of 4 items\n",
        "    self.action_memory = np.zeros((self.mem_size, n_actions), dtype=np.int64)\n",
        "\n",
        "    self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "\n",
        "    #boolean since it will be used as a mask for actor-critic in learn() \n",
        "    self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
        "\n",
        "  def store_transition(self,state ,action, reward, state_, done):\n",
        "    \n",
        "    #cleverer way to handle it, what they did in GDRL\n",
        "    index = self.mem_cntr % self.mem_size\n",
        "    self.state_memory[index] = state\n",
        "    self.action_memory[index] = action\n",
        "    self.reward_memory[index] = reward\n",
        "    self.new_state_memory[index] = state_\n",
        "    self.terminal_memory[index] = done\n",
        "    self.mem_cntr += 1\n",
        "\n",
        "  def sample_buffer(self, batch_size):\n",
        "\n",
        "    #sample only meaningful entries, i.e those that have been filled \n",
        "    max_mem = min(self.mem_cntr, self.mem_size)\n",
        "\n",
        "    batch = np.random.choice(max_mem, batch_size) #, replace=False)\n",
        "\n",
        "    states = self.state_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    next_states = self.new_state_memory[batch]\n",
        "    dones = self.terminal_memory[batch]\n",
        "\n",
        "    return states, actions, rewards, next_states, dones"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT5lYI791CkS"
      },
      "source": [
        "#https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
        "#https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
        "#https://discuss.pytorch.org/t/batch-normalization-of-linear-layers/20989\n",
        "#https://androidkt.com/use-the-batchnorm-layer-in-pytorch/\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "  #CRITIC network wants to output Q values!\n",
        "  def __init__(self, input_dims, n_actions, f1=400, f2=300 ,name=\"None\", chkpt_dir=\"None\", lr=0.001):\n",
        "    super(CriticNetwork, self).__init__()\n",
        "\n",
        "    self.input_dims = input_dims\n",
        "\n",
        "    self.checkpoint_dir = chkpt_dir\n",
        "    self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
        "\n",
        "\n",
        "    #two hidden layers means the input-400 and the 400-300 ones, right? was confused by that erlier aw well: YES, its correct\n",
        "    \n",
        "    self.h1 = nn.Linear(*input_dims, f1)\n",
        "    torch.nn.init.uniform_(self.h1.weight, -1/np.sqrt(f1), 1/np.sqrt(f1) )\n",
        "    torch.nn.init.uniform_(self.h1.bias, -1/np.sqrt(f1), 1/np.sqrt(f1) )\n",
        "\n",
        "    self.bn1 = nn.LayerNorm(f1)\n",
        "\n",
        "\n",
        "    #ALtenrative way to intitialize weights\n",
        "    #self.h1.weight.data.uniform_(-3, 3)\n",
        "\n",
        "    self.h2 = nn.Linear(f1, f2)\n",
        "    torch.nn.init.uniform_(self.h2.weight, -1/np.sqrt(f2), 1/np.sqrt(f2) )\n",
        "    torch.nn.init.uniform_(self.h2.bias, -1/np.sqrt(f2), 1/np.sqrt(f2) )   #forgot to add biases \n",
        "\n",
        "    self.bn2 = nn.LayerNorm(f2) #need batch norm on second layer as well? isn't that where we concat? wtf\n",
        "    \n",
        "    #as i thought more likely (though still not sure) output is only 1 node, the q-value of the input state\n",
        "    self.output = nn.Linear(f2, 1)\n",
        "    torch.nn.init.uniform_(self.output.weight, -3e-3, 3e-3 )\n",
        "    torch.nn.init.uniform_(self.output.bias, -3e-3, 3e-3 )\n",
        "\n",
        "\n",
        "    self.action_value = nn.Linear(n_actions, f2) #how do i need that?\n",
        "\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=0.01 )\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.to(self.device)\n",
        "\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    state = np.expand_dims(state, axis=0)\n",
        "    if not torch.is_tensor(state):\n",
        "      state = torch.tensor(state).float().to(self.device)  \n",
        "\n",
        "    if not torch.is_tensor(action):\n",
        "      action = torch.tensor(action).float().to(self.device)  \n",
        "    \n",
        "    out = F.relu(self.bn1(self.h1(state)))\n",
        "   \n",
        "   #the part I didn't understand with the action value\n",
        "    state_value =  self.bn2(self.h2(out))\n",
        "    \n",
        "    action_value = self.action_value(action)\n",
        "    \n",
        "    #I truly don't understand why that works\n",
        "    state_action_value  = F.relu(torch.add(action_value, state_value))\n",
        "\n",
        "    state_action_value = self.output(state_action_value)\n",
        "\n",
        "    return state_action_value\n",
        "\n",
        "\n",
        "  def save_checkpoint(self):\n",
        "    print(\"...saving checkpoint...\")\n",
        "    torch.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "  def load_checkpoint(self):\n",
        "    print(\"...loading checkpoint\")\n",
        "    self.load_state_dict(torch.load(self.checkpoint_file))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuMdAtWv0XDO"
      },
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "  def __init__(self, input_dims, n_actions, f1=400, f2=300 ,name=\"None\", chkpt_dir=\"None\", lr=1e-4):\n",
        "    super(ActorNetwork, self).__init__()\n",
        "\n",
        "    self.input_dims = input_dims\n",
        "\n",
        "    self.checkpoint_dir = chkpt_dir\n",
        "    self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
        "    \n",
        "    self.h1 = nn.Linear(*input_dims, f1)\n",
        "    torch.nn.init.uniform_(self.h1.weight, -1/np.sqrt(f1), 1/np.sqrt(f1) )\n",
        "    torch.nn.init.uniform_(self.h1.bias, -1/np.sqrt(f1), 1/np.sqrt(f1) )\n",
        "\n",
        "    self.bn1 = nn.LayerNorm(f1)  #LayerNorm normalizes inputs like batchNorm BUT w/out depending on batch size  \n",
        "                                   #and also seems to work better with the pytorch framework\n",
        "\n",
        "    self.h2 = nn.Linear(f1, f2)\n",
        "    torch.nn.init.uniform_(self.h2.weight, -1/np.sqrt(f2), 1/np.sqrt(f2) )\n",
        "    torch.nn.init.uniform_(self.h2.bias, -1/np.sqrt(f2), 1/np.sqrt(f2) )   #didnt forget to add biases this time\n",
        "\n",
        "    self.bn2 = nn.LayerNorm(f2) \n",
        "    \n",
        "    self.output = nn.Linear(f2, n_actions)\n",
        "    torch.nn.init.uniform_(self.output.weight, -3e-3, 3e-3 )\n",
        "    torch.nn.init.uniform_(self.output.bias, -3e-3, 3e-3 )\n",
        "\n",
        "\n",
        "\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=lr )\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.to(self.device)\n",
        "\n",
        "\n",
        "  def forward(self, state):\n",
        "    state = np.expand_dims(state, axis=0)\n",
        "    if not torch.is_tensor(state):\n",
        "      state = torch.tensor(state).float().to(self.device)  \n",
        "    out = F.relu(self.bn1(self.h1(state)))\n",
        "    out = F.relu(self.bn2(self.h2(out)))\n",
        "    \n",
        "    #should multiply this by the action bounds of my env since it is (-1,1) \n",
        "    #and env might have higher intervals, eg (-2, 2) (but not necessary in Lunar Lander)\n",
        "    #leading me to getting only part of environment's action space \n",
        "   \n",
        "    #ALSO apparently F.tanh is deprecated so we will use torch.tanh (though don't know why)\n",
        "    out = torch.tanh(self.output(out))\n",
        "    return out\n",
        "\n",
        "\n",
        "  def save_checkpoint(self):\n",
        "    print(\"...saving checkpoint...\")\n",
        "    torch.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "  def load_checkpoint(self):\n",
        "    print(\"...loading checkpoint\")\n",
        "    self.load_state_dict(torch.load(self.checkpoint_file))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yewle2kfhFcG"
      },
      "source": [
        "class DDPGAgent():\n",
        "  \n",
        "  def __init__(self, input_dims, n_actions, lr_actor= 1e-4 , lr_critic= 1e-3 , gamma=0.99, \n",
        "              f1_actor=400, f2_actor=300, f1_critic = 400, f2_critic=300,\n",
        "              name=\"None\", chkpt_dir=\"/content\", max_mem=25000, batch_size=200, tau=0.01 ):\n",
        "  \n",
        "    self.gamma  = gamma\n",
        "    self.n_actions = n_actions\n",
        "    self.input_dims = input_dims\n",
        "    self.tau = tau \n",
        "\n",
        "    self.lr_actor = lr_actor\n",
        "    self.lr_critic = lr_critic \n",
        "\n",
        "    self.f1_actor = f1_actor\n",
        "    self.f2_actor = f2_actor \n",
        "    \n",
        "\n",
        "    self.f1_critic = f1_critic\n",
        "    self.f2_critic = f2_critic \n",
        "\n",
        "    self.max_mem = max_mem\n",
        "    self.batch_size = batch_size\n",
        "    self.chkpt_dir = chkpt_dir \n",
        "    self.name = name\n",
        "\n",
        "    self.actor = ActorNetwork(self.input_dims, self.n_actions, self.f1_actor, \n",
        "                              self.f2_actor, self.name + \"_\" + \"actor\", \n",
        "                              self.chkpt_dir, lr=self.lr_actor)\n",
        "\n",
        "    self.actor_target = ActorNetwork(self.input_dims, self.n_actions, self.f1_actor, \n",
        "                              self.f2_actor, self.name + \"_\" + \"actor_target\", \n",
        "                              self.chkpt_dir, lr=self.lr_actor)\n",
        "\n",
        "\n",
        "    self.critic = CriticNetwork(self.input_dims, self.n_actions, self.f1_critic, \n",
        "                              self.f2_critic, self.name + \"_\"+ \"critic\", \n",
        "                              self.chkpt_dir, lr= self.lr_critic)\n",
        "    \n",
        "    self.critic_target = CriticNetwork(self.input_dims, self.n_actions, self.f1_critic, \n",
        "                              self.f2_critic, self.name+\"_\"+ \"critic_target\", \n",
        "                              self.chkpt_dir, lr= self.lr_critic)\n",
        "    \n",
        "    #hard copy of weights from online to target networks\n",
        "    self.update_network_parameters(tau=1)  \n",
        "\n",
        "    \n",
        "    self.memory_buffer = ReplayBuffer( self.max_mem, self.input_dims, self.n_actions)\n",
        "\n",
        "    #instantiate with zeros mean vector b/c it evolves over tiem \n",
        "    self.noise = OUActionNoise(mu=np.zeros(self.n_actions))\n",
        "\n",
        "\n",
        "  def choose_action(self, state):\n",
        "    #we don't want to calc layer norm statistics for the choose action func (since we are using batch/layer norm)\n",
        "    #and we only want to do that during our learning\n",
        "    self.actor.eval()\n",
        "\n",
        "\n",
        "    out = self.actor.forward(state).detach().numpy() + self.noise()\n",
        "    \n",
        "    self.actor.train()\n",
        "    \n",
        "    return out  #what the fuck ? why do I return an array of 4 items since it is not an action??\n",
        "                    #ANSWER: It actually is an action, we were working on the continuous environment... ....\n",
        "\n",
        "\n",
        "  def store_transition(self, state, action, reward, next_state, done):\n",
        "    self.memory_buffer.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "  def save_models(self):\n",
        "    self.actor.save_checkpoint()\n",
        "    self.actor_target.save_checkpoint()\n",
        "\n",
        "    self.critic.save_checkpoint()\n",
        "    self.critic_target.save_checkpoint()\n",
        "\n",
        "  def load_models(self):\n",
        "    self.actor.load_checkpoint()\n",
        "    self.actor_target.save_checkpoint()\n",
        "\n",
        "    self.critic.load_checkpoint()\n",
        "    self.critic_target.save_checkpoint()\n",
        "\n",
        "\n",
        "  def sample_memory(self):\n",
        "    state, action, reward, next_state, done = self.memory.sample_buffer(self.batch_size)\n",
        "    states = T.tensor(state).to(self.actor.device)\n",
        "    actions = T.tensor(action).to(self.actor.device)\n",
        "    rewards = T.tensor(reward).to(self.actor.device)\n",
        "    next_states = T.tensor(next_state).to(self.actor.device)\n",
        "    dones = T.tensor(done).to(self.actor.device)\n",
        "    return states, actions, rewards, next_states, dones\n",
        "\n",
        "  \n",
        "  #self.update_network_parameters(tau=1)  \n",
        "  def update_network_parameters(self, tau):\n",
        "    '''\n",
        "    No other arguments are necessary because they will be taken care of inside the class\n",
        "    by referencing self.model. However I will use a model argument here \n",
        "    '''  \n",
        "    state_dict_actor = self.actor.state_dict()\n",
        "    state_dict_actor_target = self.actor_target.state_dict()\n",
        "\n",
        "    state_dict_critic = self.critic.state_dict()\n",
        "    state_dict_critic_target = self.critic_target.state_dict()\n",
        "\n",
        "\n",
        "    for name, param in state_dict_actor.items():\n",
        "      transformed_param =  ( param * tau )  +  ( (1-tau) * state_dict_actor_target[name] ) \n",
        "      state_dict_actor_target[name].copy_(transformed_param)\n",
        "\n",
        "\n",
        "    for name, param in state_dict_critic.items():\n",
        "      transformed_param =  ( param * tau )  +  ( (1-tau) * state_dict_critic_target[name] ) \n",
        "      state_dict_critic_target[name].copy_(transformed_param)\n",
        "\n",
        "\n",
        "    #Generally If I would like to make it work with batch norm:\n",
        "    #self.actor_target.load_state_dict(critic_state_dict, strict=False\n",
        "    #but it is ntot really recommended because Named parameters do not include running mean and running average\n",
        "\n",
        "  def learn(self):\n",
        "    #Q: Critic, mu: Actor => WORKS \n",
        "\n",
        "    if self.memory_buffer.mem_cntr < self.batch_size:\n",
        "        return\n",
        "\n",
        "    self.actor.optimizer.zero_grad()\n",
        "    self.critic.optimizer.zero_grad()\n",
        "\n",
        "    states, actions, rewards, next_states, dones = self.memory_buffer.sample_buffer(self.batch_size)\n",
        "\n",
        "\n",
        "    #dones = torch.reshape(torch.tensor(dones), (1, 2, 1).to(self.actor.device))\n",
        "    dones = torch.reshape(torch.tensor(dones).to(self.actor.device), (1, self.batch_size, 1))\n",
        "\n",
        "\n",
        "    yi = torch.tensor(rewards).unsqueeze(1)  + self.gamma * self.critic_target.forward(next_states, self.actor_target.forward(next_states))\n",
        "\n",
        "\n",
        "    q = self.critic.forward(states, actions) \n",
        "\n",
        "    #adding the terminal states's value == 0 (keep forgetting about this SHIT)\n",
        "    q[dones] = 0.0\n",
        "\n",
        "    critic_loss = (1 / self.batch_size ) * (torch.sum((yi - q ) ** 2) )\n",
        "\n",
        "    critic_loss.backward()\n",
        "\n",
        "    self.critic.optimizer.step()\n",
        "\n",
        "    actor_loss =  torch.sum(-self.critic.forward( states, self.actor.forward(states)))  / self.batch_size  \n",
        "\n",
        "    actor_loss.backward()\n",
        "\n",
        "    self.actor.optimizer.step()\n",
        "\n",
        "\n",
        "    #needs soft param update\n",
        "    self.update_network_parameters(tau=self.tau)\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE-9berAGB3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a11bbf-51a4-4457-da63-abaf81d157ca"
      },
      "source": [
        "n_eps = 50\n",
        "agent = DDPGAgent(env.reset().shape, 2)\n",
        "scores = []\n",
        "for e in range(n_eps):\n",
        "  done, state, score = False, env.reset(), 0\n",
        "  agent.noise.reset()\n",
        "  while not done:\n",
        "    action = agent.choose_action(state)  #Do not understand how we will actually select discrete actions EDIT: it's actually \n",
        "    next_state, reward, done, _ = env.step(action.squeeze(0))    #the continuous envronmenet! I had misunderstood that \n",
        "    agent.store_transition(state, action, reward, next_state, done)\n",
        "    agent.learn()\n",
        "    score += reward\n",
        "  scores.append(score)\n",
        "  avg_score = np.mean(scores[-100: ])\n",
        "  print(f\"Episode {e} , Average 100 episode score: {avg_score}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 , Average 100 episode score: -263.51780684547657\n",
            "Episode 1 , Average 100 episode score: -222.7616051484966\n",
            "Episode 2 , Average 100 episode score: -402.22043294000355\n",
            "Episode 3 , Average 100 episode score: -414.76470857168135\n",
            "Episode 4 , Average 100 episode score: -453.3988424247817\n",
            "Episode 5 , Average 100 episode score: -426.3000763208524\n",
            "Episode 6 , Average 100 episode score: -392.13601287653256\n",
            "Episode 7 , Average 100 episode score: -408.994269055839\n",
            "Episode 8 , Average 100 episode score: -378.04496494468543\n",
            "Episode 9 , Average 100 episode score: -381.1659938734818\n",
            "Episode 10 , Average 100 episode score: -404.3421416720755\n",
            "Episode 11 , Average 100 episode score: -380.97757868492187\n",
            "Episode 12 , Average 100 episode score: -394.7040718787882\n",
            "Episode 13 , Average 100 episode score: -400.95742376171177\n",
            "Episode 14 , Average 100 episode score: -383.8160692771971\n",
            "Episode 15 , Average 100 episode score: -378.1493974056363\n",
            "Episode 16 , Average 100 episode score: -361.9074599954917\n",
            "Episode 17 , Average 100 episode score: -349.9903750299878\n",
            "Episode 18 , Average 100 episode score: -358.8205619972018\n",
            "Episode 19 , Average 100 episode score: -349.80740095966917\n",
            "Episode 20 , Average 100 episode score: -339.6586719283111\n",
            "Episode 21 , Average 100 episode score: -347.842548037039\n",
            "Episode 22 , Average 100 episode score: -367.88599442069886\n",
            "Episode 23 , Average 100 episode score: -384.7884331295915\n",
            "Episode 24 , Average 100 episode score: -386.48539458040744\n",
            "Episode 25 , Average 100 episode score: -376.85475056527696\n",
            "Episode 26 , Average 100 episode score: -400.6876413496925\n",
            "Episode 27 , Average 100 episode score: -400.65345705927655\n",
            "Episode 28 , Average 100 episode score: -399.23668543471695\n",
            "Episode 29 , Average 100 episode score: -404.55514019337085\n",
            "Episode 30 , Average 100 episode score: -412.8799567181114\n",
            "Episode 31 , Average 100 episode score: -404.4589001790549\n",
            "Episode 32 , Average 100 episode score: -414.79293740425186\n",
            "Episode 33 , Average 100 episode score: -411.9925332809717\n",
            "Episode 34 , Average 100 episode score: -403.0242100892262\n",
            "Episode 35 , Average 100 episode score: -396.7170769961425\n",
            "Episode 36 , Average 100 episode score: -386.79419143457926\n",
            "Episode 37 , Average 100 episode score: -385.40900542333065\n",
            "Episode 38 , Average 100 episode score: -378.7384771329017\n",
            "Episode 39 , Average 100 episode score: -372.3720664203188\n",
            "Episode 40 , Average 100 episode score: -366.9874660405888\n",
            "Episode 41 , Average 100 episode score: -372.9874430099094\n",
            "Episode 42 , Average 100 episode score: -367.45096809612915\n",
            "Episode 43 , Average 100 episode score: -366.5582150561674\n",
            "Episode 44 , Average 100 episode score: -368.35530226608375\n",
            "Episode 45 , Average 100 episode score: -366.6453550112435\n",
            "Episode 46 , Average 100 episode score: -372.89190531660336\n",
            "Episode 47 , Average 100 episode score: -372.48919983145373\n",
            "Episode 48 , Average 100 episode score: -372.7578977278978\n",
            "Episode 49 , Average 100 episode score: -368.3609161334823\n"
          ]
        }
      ]
    }
  ]
}